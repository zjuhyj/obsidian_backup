[深度时序公开课05：时间序列卷积的运算流程_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1QN4y1U74r?spm_id_from=333.788.videopod.sections&vd_source=f4caa78415ad4ae0c1dfa74cb0eef9ec)

---

- 一般深度学习：学习特征与标签之间的关系
- 时序学习：特征与标签、样本与样本之间（不同时间点的样本）的关系
	- 循环家族：依次处理时间点，上一时间点信息输入下一点
		- 缺陷：效率低，复杂网络过于复杂，权重冲突，长短期记忆冲突，梯度消失/爆炸等问题
	- 卷积网络：可以学习样本与样本之间的关系
	- transformer

---

## 1 卷积网络

- 点积：格式、维度需相同
- 卷积：尺度可以不同
	- 一维卷积运算：生成序列
		- ![[Pasted image 20250407154554.png]]
	- 二维卷积运算：生成矩阵
		- ![[Pasted image 20250407191752.png]]
	- 根本区别：扫描方向不同

- 卷积核
	- 值：权重w


- 为什么卷积适用于时序？
	- 大多数时序是多变量的，卷积可以跨越时间点和特征值
		- 3×3：卷积结果可以包含3个时间点3个特征的所有信息
		- 即，可以获得样本与样本之间（不同时间点的样本）、特征与样本之间、特征与特征之间以及所有信息与标签之间（loss提供）的联系

- 三维时序
	- ![[Pasted image 20250407201347.png|400]]

- 优势：
	1. 并行机制效率高。可以同时对多个样本卷积。
	2. 同一套数据有不同解读
		- 循环家族中，所有点共享权重，全部过一遍之后才会开始反向
			- 问题：所有时间点用的权重相同，不太合理
		- 卷积：可以创建多个卷积核、对同一组数据多次扫描
			- 可以有多列输出，提取出不同的维度
			- ![[Pasted image 20250407201838.png]]
	3. 卷积可天然获得多个样本信息
		- 感受野：卷积可以通过堆叠来放大感受野。堆叠的更多感受野越大，甚至可以达到循环网络同样的效果，全局感受野。




## 2 TCN

- 组成：膨胀卷积&因果卷积

### 2.1 因果卷积

- 时序处理，尤其是预测未来时刻时，不希望未来信息在当前时刻被使用。即，<u>时间上只能用过去预测未来</u>。
- 因果卷积
	- 使用填充操作，在序列开头进行填充：卷积核尺寸-1
	- ![[Pasted image 20250408090000.png]]


### 2.2 膨胀卷积

- 膨胀：通过对卷积核填上”空洞“的方式来放大感受野
- 膨胀率/膨胀指数：
	- 值为1，原有值中间填上一行0
		- ![[Pasted image 20250408090616.png|350]]
		- 覆盖的时间点变多了，最后输出时可以覆盖很长的序列
	- 值建议依次使用1、2、4
		- ![[Pasted image 20250408093211.png]]
- 为什么不直接把卷积核改成5行？
	- 会让卷积神经网络的参数量激增

## 2.3 基本计算单元：残差块


- 最基础单元：膨胀因果卷积
- TCN中的残差块
	- 左边：膨胀因果卷积（主路径）
	- 右边：残差链接（残差路径）
		- 1×1卷积用于改变维度，使得能与膨胀因果卷积的结果堆叠
	- ![[Pasted image 20250408093648.png]]
- TCN中，至少4块残差块（1+2+1，如图）
	- output并非网络输出，可以是线性层输出对应的值，也可以是其他
-  残差块串联构成TCN


## 2.4 pytorch实现

- 膨胀卷积：torch.nn.Conv1d实现
	- pytorc中一维卷积和二维卷积参数相同，但含义不同。







## 3 补充：残差块


- [经典的卷积神经网络模型 - ResNet_瓶颈残差模块-CSDN博客](https://blog.csdn.net/flyfish1986/article/details/140117215)